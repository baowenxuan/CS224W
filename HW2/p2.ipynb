{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Node Embeddings with TransE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Warmup: Why the Comparative Loss?\n",
    "We could have a graph with 4 entities and 2 relationships:\n",
    "```\n",
    "(Higher Brothers) ---(from)--->   (China)  \n",
    "                                     |  \n",
    "                              (to the north of)    \n",
    "                                     |  \n",
    "                                     v  \n",
    "   (Rich Brian)   ---(from)---> (Indonesia)  \n",
    "```\n",
    "There is a trival solution than could minimize the loss function to 0, that is:  \n",
    "$\\mathbf e = [0, 1]^T$ for all entities and $\\boldsymbol \\ell = [0, 0]^T$ for all relationships. This is a completely useless embedding. \n",
    "\n",
    "Note that is does not mean that the algorithm will always end up with this embedding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 The Purpose of the Margin\n",
    "With the same embeddings in 2.1, we again minimize the loss function to 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Why are Entity Embeddings Normalized?\n",
    "The paper explains: \n",
    "> This constraint is important for our model, as it is for previous embedding-based methods, because it prevents the training process to trivially minimize $\\mathcal L$ by artificially increasing entity embeddings norms. \n",
    "\n",
    "Removing the norm constraint, the margin may sometimes be useless. Assuming that the loss function could be minimized to zero with a small non-zero margin $\\gamma_1$ (which means that we get a perfect embedding). Then, even if we increase the margin (from $\\gamma_1$ to $\\gamma_2$), the model could cheat the loss function by just increase the norms (while keeping the direction) by $\\frac{\\gamma_2}{\\gamma_1}$. The model minimizes the loss to zero, but does not improve the embeddings. \n",
    "\n",
    "Note that this is not always the case. If we cannot get a perfect embedding, the model will not always increase the norm otherwise the loss will increase finally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Where TransE fails\n",
    "Still the same graph. If we have:  \n",
    "$\\mathbf u_0 + \\boldsymbol \\ell_0 = \\mathbf u_1, \\mathbf u_2 + \\boldsymbol \\ell_1 = \\mathbf u_0, \n",
    "\\mathbf u_3 + \\boldsymbol \\ell_1 = \\mathbf u_1$. \n",
    "\n",
    "Then we got\n",
    "$\\mathbf u_2 + \\boldsymbol \\ell_0 = \\mathbf u_3$\n",
    "However, $(u_2, \\ell_0, v_3) \\notin S$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Implementation of TransE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}_{\\text{simple}}=\\sum_{(h, \\ell, t) \\in S} d(\\mathbf{h}+\\boldsymbol{\\ell}, \\mathbf{t})\n",
    "\\end{aligned}\n",
    "$$\n",
    "For each triplet $(h, \\ell, t)$: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{d} &= d(\\mathbf{h}+\\boldsymbol{\\ell}, \\mathbf{t}) \\\\\n",
    "d &= \\sqrt{\\mathbf{d}^T \\mathbf{d}} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "Therefore: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial d}{\\partial \\mathbf{d}} &= \\frac{2 \\mathbf{d}}{2 \\sqrt{\\mathbf{d}^T \\mathbf{d}}} = \\frac{\\mathbf{d}}{d} \\\\\n",
    "\\frac{\\partial d}{\\partial \\mathbf{h}} &= \\frac{\\partial d}{\\partial \\boldsymbol{\\ell}} = \\frac{\\mathbf{d}}{d} \\\\\n",
    "\\frac{\\partial d}{\\partial \\mathbf{t}} &= -\\frac{\\mathbf{d}}{d}\n",
    "\\end{aligned}\n",
    "$$\n",
    "Then for a batch, we just need to sum up the gradient for each sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple gradient only consider the existed triplets\n",
    "def gradient_simple(h, l, t):\n",
    "    d = h + l - t\n",
    "    dh = d / np.linalg.norm(d)\n",
    "    dl = dh\n",
    "    dt = - dh\n",
    "    return dh, dl, dt, np.linalg.norm(d)\n",
    "\n",
    "# comparative gradient, with or without margin\n",
    "def gradient_comparative(h, l, t, hp, tp, margin=0.0):\n",
    "    dh1, dl1, dt1, d1 = gradient_simple(h, l, t)\n",
    "    dh2, dl2, dt2, d2 = gradient_simple(hp, l, tp)\n",
    "    if margin + d1 - d2 <= 0:  # correct\n",
    "        return np.zeros_like(dh1), np.zeros_like(dl1), np.zeros_like(dt1), 0\n",
    "    else:  # wrong\n",
    "        return dh1 - dh2, dl1 - dl2, dt1 - dt2, margin + d1 - d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample(h_i, t_i, n_e):\n",
    "    p_i = np.random.choice(n_e)\n",
    "    if np.random.choice(2):  # resample t\n",
    "        return h_i, p_i\n",
    "    else:  # resample h\n",
    "        return p_i, t_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TransE(Entities, Relationships, Edges, k=2, loss='comparative', alpha=0.01, epochs=100, sample_rate=1.0, margin=1.0):\n",
    "    n_e = len(Entities)  # number of entities\n",
    "    n_l = len(Relationships)  # number of relationships\n",
    "    m = len(Edges)  # number of edges\n",
    "    b = int(sample_rate * m)  # number of edges sampled in each epoch\n",
    "    \n",
    "    # Initialize Embeddings\n",
    "    # Relationships Embeddings\n",
    "    L = np.random.uniform(low=-6/np.sqrt(k), high=6/np.sqrt(k), size=(n_l, k))\n",
    "    L = L / np.linalg.norm(L, axis=1, keepdims=True)\n",
    "    # Entities Embeddings\n",
    "    E = np.random.uniform(low=-6/np.sqrt(k), high=6/np.sqrt(k), size=(n_e, k))\n",
    "    \n",
    "    # Initialize their gradients\n",
    "    dL = np.zeros_like(L)\n",
    "    dE = np.zeros_like(E)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Normalize Entity Embeddings\n",
    "        # E = E / np.linalg.norm(E, axis=1, keepdims=True)\n",
    "        # Clear gradients\n",
    "        dL.fill(0.0)\n",
    "        dE.fill(0.0)\n",
    "        sum_loss = 0\n",
    "        # Sample batch of size b\n",
    "        batch = np.random.choice(m, b, replace=False)\n",
    "        for index in batch:\n",
    "            h_i, l_i, t_i = Edges[index]  # positive triplets\n",
    "            hp_i, tp_i = random_sample(h_i, t_i, n_e)  # corrupted triplets\n",
    "            if loss == 'simple':\n",
    "                h, l, t = E[h_i], L[l_i], E[t_i]\n",
    "                dh, dl, dt, d = gradient_simple(h, l, t)\n",
    "                # accumulate gradients\n",
    "                dE[h_i] = dE[h_i] + dh\n",
    "                dL[l_i] = dL[l_i] + dl\n",
    "                dE[t_i] = dE[t_i] + dt\n",
    "                sum_loss = sum_loss + d\n",
    "            elif loss == 'comparative':\n",
    "                h, l, t = E[h_i], L[l_i], E[t_i]\n",
    "                hp, tp = E[hp_i], E[tp_i]\n",
    "                dh, dl, dt, d = gradient_comparative(h, l, t, hp, tp, margin=2.0)\n",
    "                # accumulate gradients\n",
    "                dE[h_i] = dE[h_i] + dh\n",
    "                dL[l_i] = dL[l_i] + dl\n",
    "                dE[t_i] = dE[t_i] + dt\n",
    "                sum_loss = sum_loss + d\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "        E = E - alpha * dE \n",
    "        L = L - alpha * dL\n",
    "        if epoch % 50 == 0:\n",
    "            print('Epoch', epoch, ':', sum_loss)\n",
    "        \n",
    "    # E = E / np.linalg.norm(E, axis=1, keepdims=True)\n",
    "    return E, L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "China to the north of Indonesia\n",
      "Higher Brothers from China\n",
      "Rich Brian from Indonesia\n"
     ]
    }
   ],
   "source": [
    "# Simple Graph\n",
    "Entities = {0: 'China', 1: 'Indonesia', 2: 'Higher Brothers', 3: 'Rich Brian'}\n",
    "Relationships = {0: 'to the north of', 1: 'from'}\n",
    "Edges = [(0, 0, 1), (2, 1, 0), (3, 1, 1)]\n",
    "for h_i, l_i, t_i in Edges:\n",
    "    print(Entities[h_i], Relationships[l_i], Entities[t_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : 10.332187792774103\n",
      "Epoch 50 : 13.425523543179683\n",
      "Epoch 100 : 10.620780422533668\n",
      "Epoch 150 : 6.661666409476698\n",
      "Epoch 200 : 8.25368831727983\n",
      "Epoch 250 : 7.6690717218380335\n",
      "Epoch 300 : 10.380212460687357\n",
      "Epoch 350 : 6.39088840592918\n",
      "Epoch 400 : 5.25310525184958\n",
      "Epoch 450 : 6.095685087100951\n",
      "Epoch 500 : 7.172016782605932\n",
      "Epoch 550 : 6.7771743831426985\n",
      "Epoch 600 : 4.478209156781671\n",
      "Epoch 650 : 8.714961684796366\n",
      "Epoch 700 : 4.429065031929193\n",
      "Epoch 750 : 4.128495337528484\n",
      "Epoch 800 : 5.3099327007962005\n",
      "Epoch 850 : 5.870670534028997\n",
      "Epoch 900 : 5.256516900549345\n",
      "Epoch 950 : 2.5849615817430704\n"
     ]
    }
   ],
   "source": [
    "E, L = TransE(Entities, Relationships, Edges, k=2, loss='comparative', alpha=0.005, epochs=1000, sample_rate=1.0, margin=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.58017788,  0.90863433],\n",
       "       [-1.11345653,  1.72820179],\n",
       "       [ 0.64104502, -1.03172   ],\n",
       "       [-0.80101233,  2.09512848]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.91502093,  0.86074989],\n",
       "       [-1.58774833,  1.82029599]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snap",
   "language": "python",
   "name": "snap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
